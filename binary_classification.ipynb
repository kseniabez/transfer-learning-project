{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be49b13c",
   "metadata": {},
   "source": [
    "# Basic Project: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:16.627531Z",
     "start_time": "2025-05-02T11:50:16.623822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9428c9",
   "metadata": {},
   "source": [
    "Tic() Toc() Functions to track training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7462599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tic():\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "def toc():\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        return time.time() - startTime_for_tictoc\n",
    "        #print(\"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\")\n",
    "    else:\n",
    "        print(\"Toc: start time not set\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d07f735d6ce8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:26.163270Z",
     "start_time": "2025-05-02T11:50:26.159291Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = \"datasets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef44fd",
   "metadata": {},
   "source": [
    "Download ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d822d2ee3a2ca1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:28.100014Z",
     "start_time": "2025-05-02T11:50:28.092453Z"
    }
   },
   "outputs": [],
   "source": [
    "# for ResNet18:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613107a2",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666abda14682e0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:29.995502Z",
     "start_time": "2025-05-02T11:50:29.959823Z"
    }
   },
   "outputs": [],
   "source": [
    "trainval_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types='binary-category',\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "test_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='test',\n",
    "        target_types='binary-category',\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f852f9",
   "metadata": {},
   "source": [
    "Train / Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c91e21781a76757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:31.831977Z",
     "start_time": "2025-05-02T11:50:31.824990Z"
    }
   },
   "outputs": [],
   "source": [
    "val_ratio = 0.2  # 20% for validation\n",
    "train_size = int((1 - val_ratio) * len(trainval_data))\n",
    "val_size = len(trainval_data) - train_size\n",
    "\n",
    "train_data, val_data = random_split(trainval_data, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5da3fd74362f472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:34.049178Z",
     "start_time": "2025-05-02T11:50:34.043776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 2944, 3669)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data), len(train_data), len(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816e8f8",
   "metadata": {},
   "source": [
    "Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6981347e5d5a52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:39.097131Z",
     "start_time": "2025-05-02T11:50:39.090121Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41503d58af5b6db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:53.783971Z",
     "start_time": "2025-05-02T11:50:53.779321Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['val'] = val_loader\n",
    "dataloaders['test'] = test_loader\n",
    "\n",
    "dataset_sizes['train'] = len(train_data)\n",
    "dataset_sizes['val'] = len(val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33cf94",
   "metadata": {},
   "source": [
    "Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72b91a7187a97c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T11:50:55.634513Z",
     "start_time": "2025-05-02T11:50:55.446129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cpu':\n",
    "    print(\"If GPU is available: \\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"Restart the kernel and run the code again.\")\n",
    "    print(\"Check with `print(torch.cuda.is_available())`\")\n",
    "    print(\"Documentation: https://pytorch.org/get-started/locally/\")\n",
    "    \n",
    "# ResNet18\n",
    "network = models.resnet18(weights='DEFAULT')\n",
    "nf = network.fc.in_features\n",
    "network.fc = nn.Linear(nf, 2)\n",
    "network = network.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff8116",
   "metadata": {},
   "source": [
    "Function to Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5dcf515323571c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:08:02.047077Z",
     "start_time": "2025-05-02T12:08:02.038085Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_network(network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=25):\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Set Starting Time\n",
    "    tic()\n",
    "\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_network_params_path = os.path.join(tempdir, 'best_network_params.pt')\n",
    "\n",
    "        torch.save(network.state_dict(), best_network_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    network.train()\n",
    "                else:\n",
    "                    network.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for X, Y in dataloaders[phase]:\n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        S = network(X)\n",
    "                        _, P = torch.max(S, 1)\n",
    "                        loss = criterion(S, Y)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * X.size(0)\n",
    "                    running_corrects += torch.sum(P == Y.data)\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_accuracies.append(epoch_acc.item())\n",
    "                else:\n",
    "                    val_losses.append(epoch_loss)\n",
    "                    val_accuracies.append(epoch_acc.item())\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(network.state_dict(), best_network_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        network.load_state_dict(torch.load(best_network_params_path, weights_only=True))\n",
    "        \n",
    "    # Print Time for Training only\n",
    "    el_time_training = toc()\n",
    "    print(f\"\\nTime for training: {el_time_training:.1f} sec.\")\n",
    "    \n",
    "    # Return Network and Training Statistics\n",
    "    train_stats = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'elapsed_time': el_time_training,\n",
    "    }\n",
    "    \n",
    "    return network, train_stats\n",
    "    # return network, train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12c171e7ac4cab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:08:27.126364Z",
     "start_time": "2025-05-02T12:08:27.121461Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(network, loader, print_result=True):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            S = network(X)\n",
    "            _, P = torch.max(S, 1)\n",
    "            correct += (P == Y).sum().item()\n",
    "            total += Y.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    if print_result:\n",
    "        print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "        \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1698ab4",
   "metadata": {},
   "source": [
    "Define Entropy Criterion and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d21dbcfa1a6a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:08:28.552107Z",
     "start_time": "2025-05-02T12:08:28.546457Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8828229",
   "metadata": {},
   "source": [
    "Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11eb39d6f2d80808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:19:22.161337Z",
     "start_time": "2025-05-02T12:08:36.603934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running for 25 epochs\n",
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    network, train_stats = train_network(\n",
    "        network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=25\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd0e1f",
   "metadata": {},
   "source": [
    "Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f41b5c7346cd86a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T12:43:48.473953Z",
     "start_time": "2025-05-02T12:43:48.255739Z"
    }
   },
   "outputs": [],
   "source": [
    "def VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "    plt.figure(facecolor='white', figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.figure(facecolor='white')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    #plt.ylim([0.9, 1.02])\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Visualize the loss and accuracy of the Network\n",
    "if TrainYN:    \n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    compute_accuracy(network, test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e003",
   "metadata": {},
   "source": [
    "## Multi-Class Problem\n",
    "- Identifying all 37 breeds of Cats & Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5835050d7c3d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_TrainTestData(data_root, target_types, transform):\n",
    "    trainval_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "    test_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='test',\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "\n",
    "    val_ratio = 0.2  # 20% for validation\n",
    "    train_size = int((1 - val_ratio) * len(trainval_data))\n",
    "    val_size = len(trainval_data) - train_size\n",
    "\n",
    "    train_data, val_data = random_split(trainval_data, [train_size, val_size])\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def DataLoaderFnc(train_data, val_data, test_data, batch_size=32):\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataset_sizes = {}\n",
    "\n",
    "    dataloaders['train'] = train_loader\n",
    "    dataloaders['val'] = val_loader\n",
    "    dataloaders['test'] = test_loader\n",
    "\n",
    "    dataset_sizes['train'] = len(train_data)\n",
    "    dataset_sizes['val'] = len(val_data)\n",
    "\n",
    "    return dataloaders, dataset_sizes\n",
    "\n",
    "\n",
    "def Initialize_ResNet18(no_target_classes=2):\n",
    "    \n",
    "    network = models.resnet18(weights='DEFAULT')\n",
    "    nf = network.fc.in_features\n",
    "    network.fc = nn.Linear(nf, no_target_classes)\n",
    "    network = network.to(device)\n",
    "    \n",
    "    return network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c03be1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, 'category', transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size=32)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes=37)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the network\n",
    "    network, train_stats = train_network(\n",
    "        init_network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=10\n",
    "    )\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']\n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3153c621",
   "metadata": {},
   "source": [
    "Build one big Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97372441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainResNet18_S1(data_root, target_types, transform, TrainParams):\n",
    "    \n",
    "    # Extract Training Parameters\n",
    "    batch_size = TrainParams.get('batch_size', 32)\n",
    "    num_epochs = TrainParams.get('num_epochs', 25)\n",
    "    no_target_classes = TrainParams.get('no_target_classes', 2)\n",
    "    lr = TrainParams.get('lr', 1e-4)\n",
    "    L = TrainParams.get('L', 0)  # Number of layers to fine-tune simultaneously\n",
    "    strategy = TrainParams['strategy']  # 'fine-tune' or 'un-freeze'\n",
    "    curr_layer = TrainParams.get('curr_layer', 0)  # Current layer to unfreeze\n",
    "    InitNetYN = TrainParams.get('InitNetYN', True)  # Initialize network\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, target_types, transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes)\n",
    "    \n",
    "    # Freeze/Unfreeze Layers\n",
    "    if strategy == 'fine-tune':\n",
    "        for l, param in enumerate(init_network.parameters()):\n",
    "            if l <= L:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    elif strategy == 'un-freeze':\n",
    "        for l, param in enumerate(init_network.parameters()):\n",
    "            param.requires_grad = False\n",
    "            if l == curr_layer:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr)\n",
    "\n",
    "    # Train the network\n",
    "    network, train_stats = train_network(\n",
    "        init_network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs\n",
    "    )\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n",
    "\n",
    "    # Add the final accuracy to the training statistics\n",
    "    train_stats['final_accuracy'] = final_acc\n",
    "    \n",
    "    return network, train_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ae48a",
   "metadata": {},
   "source": [
    "### Strategy 1: Fine-tune $l$ layers simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08d49cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers to unfreeze\n",
    "layers = [0, 3, 5, 10]\n",
    "\n",
    "# Training Parameters\n",
    "TrainParams = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'no_target_classes': 37,\n",
    "    'lr': 1e-4,\n",
    "    'L': 0,  # Unfreeze the last layer\n",
    "    'strategy': 'fine-tune',  # 'fine-tune' or 'un-freeze'\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through the layers and train the network\n",
    "train_stats_list = []\n",
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    for l in layers:\n",
    "        print(f\"\\nTraining with fine-tuning layers up to layer {l}...\")\n",
    "        TrainParams['L'] = l  # Set the number of layers to fine-tune\n",
    "        _, train_stats_S1 = TrainResNet18_S1(data_root, 'category', transform, TrainParams)\n",
    "        print(f\"Finished training with fine-tuning layers up to layer {l}.\")\n",
    "        train_stats_list.append(train_stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38908dd3",
   "metadata": {},
   "source": [
    "### Strategy 2: Gradual un-freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainResNet18_S2(data_root, target_types, transform, TrainParams):\n",
    "    \n",
    "    # Extract Training Parameters\n",
    "    batch_size = TrainParams.get('batch_size', 32)\n",
    "    num_epochs = TrainParams.get('num_epochs', 25)\n",
    "    no_target_classes = TrainParams.get('no_target_classes', 2)\n",
    "    lr = TrainParams.get('lr', 1e-4)\n",
    "    L = TrainParams.get('L', 0)  # Number of layers to fine-tune simultaneously\n",
    "    strategy = TrainParams['strategy']  # 'fine-tune' or 'un-freeze'\n",
    "    curr_layer = TrainParams.get('curr_layer', 0)  # Current layer to unfreeze\n",
    "    InitNetYN = TrainParams.get('InitNetYN', True)  # Initialize network\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, target_types, transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr)\n",
    "    \n",
    "    # Freeze/Unfreeze Layers\n",
    "    network = init_network\n",
    "    list_train_stats = []\n",
    "    print(\"\\nStart Training Network (Strategy: gradually unfreeze layers) ...\")\n",
    "    # Loop around the layers and train the network\n",
    "    for layer, _ in enumerate(init_network.parameters()):\n",
    "        # Freeze / unfreeze the right layers (gradually unfreeze)\n",
    "        for l, param in enumerate(network.parameters()):\n",
    "            param.requires_grad = False\n",
    "            if l == layer: # unfreeze the current layer\n",
    "                param.requires_grad = True\n",
    "        # Train the network (only the unfreezed layers)\n",
    "        print(f\"Training with unfreezing layer {layer}...\")\n",
    "        network, train_stats = train_network(\n",
    "            network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs\n",
    "        )\n",
    "        list_train_stats.append(train_stats)\n",
    "\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n",
    "\n",
    "    # Add the final accuracy to the training statistics\n",
    "    train_stats['final_accuracy'] = final_acc\n",
    "    \n",
    "    return network, train_stats, list_train_stats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271931e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "TrainParams['strategy'] = 'un-freeze'\n",
    "\n",
    "# Train the network with gradually unfreezing layers\n",
    "TrainYN = True\n",
    "if TrainYN:\n",
    "    trained_net_S2, train_stats_S2, list_train_stats = TrainResNet18_S2(data_root, 'category', transform, TrainParams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a005a78",
   "metadata": {},
   "source": [
    "## Fine-Tuning with imbalanced classes\n",
    "- Check the Training behavior with class-imbalance\n",
    "- Try a strategy (e.g. weighted cross-entropy and/or over-sampling of minority classes) to compensate the imbalanced training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
