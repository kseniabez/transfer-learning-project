{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic Project: Transfer Learning",
   "id": "12cd252a493a0ee7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import numpy as np\n",
    "from torchvision.transforms import RandAugment\n",
    "import random \n",
    "import PIL\n",
    "import pandas as pd\n"
   ],
   "id": "7fc03604f11b73e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tic() Toc() Functions to track training time",
   "id": "afcea887190e43c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tic():\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "def toc():\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        return time.time() - startTime_for_tictoc\n",
    "        #print(\"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\")\n",
    "    else:\n",
    "        print(\"Toc: start time not set\")\n",
    "        "
   ],
   "id": "91250fbd7af82c01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_root = \"datasets\"\n",
   "id": "d7b73093633a402a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Download ResNet18",
   "id": "c921640e5fe66713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# for ResNet18:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ],
   "id": "f0c769af9192f030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset",
   "id": "6ffded93cb188357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainval_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types='binary-category',\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "test_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='test',\n",
    "        target_types='binary-category',\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n"
   ],
   "id": "184d7a7fb08dd291"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train / Test Dataset Split",
   "id": "d496a91f75bc6259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_ratio = 0.2  # 20% for validation\n",
    "train_size = int((1 - val_ratio) * len(trainval_data))\n",
    "val_size = len(trainval_data) - train_size\n",
    "\n",
    "train_data, val_data = random_split(trainval_data, [train_size, val_size])\n"
   ],
   "id": "5983a39a4c3f9af2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize 10 random images",
   "id": "d21781df5cb2ded0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(val_data), len(train_data), len(test_data)\n",
    "\n",
    "visualize_dataset = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types='category',\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "images2vis, _ = random_split(visualize_dataset, [train_size, val_size])\n",
    "\n",
    "def de_normalize(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    if isinstance(img_tensor, tuple):\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.clone() # avoid modifying the original tensor\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor\n",
    "\n",
    "def VisImages(data):\n",
    "    \n",
    "    # Randomly select 10 indices\n",
    "    indices = random.sample(range(len(data)), 10)\n",
    "\n",
    "    # Extract the corresponding images and labels\n",
    "    images = [de_normalize(data[i][0]) for i in indices]\n",
    "    labels = [data.dataset.classes[data[i][1]] for i in indices]\n",
    "\n",
    "    # Convert tensors to PIL images\n",
    "    images = [F.to_pil_image(img) for img in images]\n",
    "\n",
    "    # Display the images and labels\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    for ax, img, label in zip(axes.flatten(), images, labels):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{label}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize \n",
    "VisImages(images2vis)\n"
   ],
   "id": "49b6c742c123d77f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Loaders",
   "id": "6a3feffb781110f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "788002d1add3189b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['val'] = val_loader\n",
    "dataloaders['test'] = test_loader\n",
    "\n",
    "dataset_sizes['train'] = len(train_data)\n",
    "dataset_sizes['val'] = len(val_data)\n"
   ],
   "id": "24987ec0f9096492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize the Network",
   "id": "634804468a4f3632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cpu':\n",
    "    print(\"If GPU is available: \\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"Restart the kernel and run the code again.\")\n",
    "    print(\"Check with `print(torch.cuda.is_available())`\")\n",
    "    print(\"Documentation: https://pytorch.org/get-started/locally/\")\n",
    "    \n",
    "# ResNet18\n",
    "init_network = models.resnet18(weights='DEFAULT')\n",
    "nf = init_network.fc.in_features\n",
    "init_network.fc = nn.Linear(nf, 2)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in init_network.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the final layer\n",
    "for param in init_network.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "init_network = init_network.to(device)\n"
   ],
   "id": "7ff06bc557b21b7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to Train the Network",
   "id": "efaa912a8d98aa37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_network(network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=25):\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def evaluate_phase(phase):\n",
    "        \"\"\"Helper to evaluate model on a given phase.\"\"\"\n",
    "        network.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, Y in dataloaders[phase]:\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                S = network(X)\n",
    "                _, P = torch.max(S, 1)\n",
    "                loss = criterion(S, Y)\n",
    "\n",
    "                running_loss += loss.item() * X.size(0)\n",
    "                running_corrects += torch.sum(P == Y.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "        return epoch_loss, epoch_acc.item()\n",
    "\n",
    "    # Set Starting Time\n",
    "    tic()\n",
    "\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_network_params_path = os.path.join(tempdir, 'best_network_params.pt')\n",
    "\n",
    "        torch.save(network.state_dict(), best_network_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        # Step 0: Evaluate before training\n",
    "        print(\"Step 0 (Before Training)\")\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            loss, acc = evaluate_phase(phase)\n",
    "            if phase == 'train':\n",
    "                train_losses.append(loss)\n",
    "                train_accuracies.append(acc)\n",
    "            else:\n",
    "                val_losses.append(loss)\n",
    "                val_accuracies.append(acc)\n",
    "            print(f'{phase} Loss: {loss:.4f} Acc: {acc:.4f}')\n",
    "        print()\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    network.train()\n",
    "                else:\n",
    "                    network.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for X, Y in dataloaders[phase]:\n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        S = network(X)\n",
    "                        _, P = torch.max(S, 1)\n",
    "                        loss = criterion(S, Y)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * X.size(0)\n",
    "                    running_corrects += torch.sum(P == Y.data)\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_accuracies.append(epoch_acc.item())\n",
    "                else:\n",
    "                    val_losses.append(epoch_loss)\n",
    "                    val_accuracies.append(epoch_acc.item())\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(network.state_dict(), best_network_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        network.load_state_dict(torch.load(best_network_params_path, weights_only=True))\n",
    "        \n",
    "    # Print Time for Training only\n",
    "    el_time_training = toc()\n",
    "    print(f\"\\nTime for training: {el_time_training:.1f} sec.\")\n",
    "    \n",
    "    # Return Network and Training Statistics\n",
    "    train_stats = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'elapsed_time': el_time_training,\n",
    "    }\n",
    "    \n",
    "    return network, train_stats\n",
    "    # return network, train_losses, val_losses, train_accuracies, val_accuracies\n"
   ],
   "id": "9cb98059a424bc82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_accuracy(network, loader, print_result=True):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            S = network(X)\n",
    "            _, P = torch.max(S, 1)\n",
    "            correct += (P == Y).sum().item()\n",
    "            total += Y.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    if print_result:\n",
    "        print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "        \n",
    "    return acc\n"
   ],
   "id": "24cf58fc184ad5a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Entropy Criterion and the Optimizer",
   "id": "e76b352251de29ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(init_network.parameters(), lr=1e-4)\n"
   ],
   "id": "77cf9ce57555e869"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the Network",
   "id": "f9899b734d37fa0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Running for n epochs\n",
    "num_epochs = 100\n",
    "TrainYN = True\n",
    "if TrainYN:\n",
    "    trained_network, train_stats = train_network(\n",
    "        init_network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=num_epochs\n",
    "    )\n"
   ],
   "id": "787bfb4c92f53f4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the Results",
   "id": "f5f96100aa1539ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "    plt.figure(facecolor='white', figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlim(0, num_epochs)\n",
    "    # plt.title('Loss over epochs')\n",
    "    plt.grid()\n",
    "\n",
    "    #plt.figure(facecolor='white')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim(0, num_epochs)\n",
    "    #plt.ylim([0.9, 1.02])\n",
    "    # plt.title('Accuracy over epochs')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Visualize the loss and accuracy of the Network\n",
    "if TrainYN:    \n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    compute_accuracy(trained_network, test_loader)\n",
    "    "
   ],
   "id": "b2fdeed91f5b83f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Class Problem\n",
    "- Identifying all 37 breeds of Cats & Dogs"
   ],
   "id": "61fc89052f19f68f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def Load_TrainTestData(data_root, target_types, transform):\n",
    "    trainval_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "    test_data = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='test',\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "\n",
    "    val_ratio = 0.2  # 20% for validation\n",
    "    train_size = int((1 - val_ratio) * len(trainval_data))\n",
    "    val_size = len(trainval_data) - train_size\n",
    "\n",
    "    train_data, val_data = random_split(trainval_data, [train_size, val_size])\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def DataLoaderFnc(train_data, val_data, test_data, batch_size=32):\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataset_sizes = {}\n",
    "\n",
    "    dataloaders['train'] = train_loader\n",
    "    dataloaders['val'] = val_loader\n",
    "    dataloaders['test'] = test_loader\n",
    "\n",
    "    dataset_sizes['train'] = len(train_data)\n",
    "    dataset_sizes['val'] = len(val_data)\n",
    "\n",
    "    return dataloaders, dataset_sizes\n",
    "\n",
    "\n",
    "def Initialize_ResNet18(no_target_classes=2):\n",
    "    \n",
    "    network = models.resnet18(weights='DEFAULT')\n",
    "    nf = network.fc.in_features\n",
    "    network.fc = nn.Linear(nf, no_target_classes)\n",
    "    network = network.to(device)\n",
    "    \n",
    "    return network\n",
    "\n"
   ],
   "id": "98201e85fcd20326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, 'category', transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size=32)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes=37)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the network\n",
    "    network, train_stats = train_network(\n",
    "        init_network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs=10\n",
    "    )\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']\n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n"
   ],
   "id": "7a4b85df6d1dd343"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build one big Training Function",
   "id": "c45e63bc4596213f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def TrainResNet18_S1(data_root, target_types, transform, TrainParams):\n",
    "    \n",
    "    # Extract Training Parameters\n",
    "    batch_size = TrainParams.get('batch_size', 32)\n",
    "    num_epochs = TrainParams.get('num_epochs', 25)\n",
    "    no_target_classes = TrainParams.get('no_target_classes', 2)\n",
    "    lr = TrainParams.get('lr', 1e-4)\n",
    "    L = TrainParams.get('L', 0)  # Number of layers to fine-tune simultaneously\n",
    "    strategy = TrainParams['strategy']  # 'fine-tune' or 'un-freeze'\n",
    "    curr_layer = TrainParams.get('curr_layer', 0)  # Current layer to unfreeze\n",
    "    InitNetYN = TrainParams.get('InitNetYN', True)  # Initialize network\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, target_types, transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes)\n",
    "    \n",
    "    # Freeze/Unfreeze Layers\n",
    "    if strategy == 'fine-tune':\n",
    "        for l, param in enumerate(init_network.parameters()):\n",
    "            if l <= L:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    elif strategy == 'un-freeze':\n",
    "        for l, param in enumerate(init_network.parameters()):\n",
    "            param.requires_grad = False\n",
    "            if l == curr_layer:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr)\n",
    "\n",
    "    # Train the network\n",
    "    network, train_stats = train_network(\n",
    "        init_network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs\n",
    "    )\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n",
    "\n",
    "    # Add the final accuracy to the training statistics\n",
    "    train_stats['final_accuracy'] = final_acc\n",
    "    \n",
    "    return network, train_stats\n",
    "    "
   ],
   "id": "70ff9cf1caae7c2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Strategy 1: Fine-tune $l$ layers simultaneously",
   "id": "2eecdce443904e53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Layers to unfreeze\n",
    "layers = [0, 3, 5, 10]\n",
    "\n",
    "# Training Parameters\n",
    "TrainParams = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'no_target_classes': 37,\n",
    "    'lr': 1e-4,\n",
    "    'L': 0,  # Unfreeze the last layer\n",
    "    'strategy': 'fine-tune',  # 'fine-tune' or 'un-freeze'\n",
    "}\n",
    "\n",
    "\n",
    "# Loop through the layers and train the network\n",
    "train_stats_list = []\n",
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    for l in layers:\n",
    "        print(f\"\\nTraining with fine-tuning layers up to layer {l}...\")\n",
    "        TrainParams['L'] = l  # Set the number of layers to fine-tune\n",
    "        _, train_stats_S1 = TrainResNet18_S1(data_root, 'category', transform, TrainParams)\n",
    "        print(f\"Finished training with fine-tuning layers up to layer {l}.\")\n",
    "        train_stats_list.append(train_stats)\n",
    "    "
   ],
   "id": "6b2bbf1ddec0aff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Strategy 2: Gradual un-freezing",
   "id": "23f845d263fd17ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def TrainResNet18_S2(data_root, target_types, transform, TrainParams):\n",
    "    \n",
    "    # Extract Training Parameters\n",
    "    batch_size = TrainParams.get('batch_size', 32)\n",
    "    num_epochs = TrainParams.get('num_epochs', 25)\n",
    "    no_target_classes = TrainParams.get('no_target_classes', 2)\n",
    "    lr = TrainParams.get('lr', 1e-4)\n",
    "    \n",
    "    # Load Train, Validation and Test Data\n",
    "    train_data, val_data, test_data = Load_TrainTestData(data_root, target_types, transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloaders, dataset_sizes = DataLoaderFnc(train_data, val_data, test_data, batch_size)\n",
    "\n",
    "    # Initialize ResNet18\n",
    "    init_network = Initialize_ResNet18(no_target_classes)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(init_network.parameters(), lr)\n",
    "    \n",
    "    # Freeze/Unfreeze Layers\n",
    "    network = init_network\n",
    "    list_train_stats = []\n",
    "    print(\"\\nStart Training Network (Strategy: gradually unfreeze layers) ...\")\n",
    "    # Loop around the layers and train the network\n",
    "    for layer, _ in enumerate(init_network.parameters()):\n",
    "        # Freeze / unfreeze the right layers (gradually unfreeze)\n",
    "        for l, param in enumerate(network.parameters()):\n",
    "            param.requires_grad = False\n",
    "            if l == layer: # unfreeze the current layer\n",
    "                param.requires_grad = True\n",
    "        # Train the network (only the unfreezed layers)\n",
    "        print(\"------------------------------------------\")\n",
    "        print(f\"\\nTraining with unfreezing layer {layer}...\")\n",
    "        network, train_stats = train_network(\n",
    "            network, dataloaders, dataset_sizes, criterion, optimizer, num_epochs\n",
    "        )\n",
    "        list_train_stats.append(train_stats)\n",
    "\n",
    "\n",
    "    # Visualize the loss and accuracy of the Network\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = \\\n",
    "                train_stats['train_losses'], train_stats['val_losses'], \\\n",
    "                train_stats['train_accuracies'], train_stats['val_accuracies']  \n",
    "    VisLossAccuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # Print the Accuracy\n",
    "    final_acc = compute_accuracy(network, test_loader, print_result=True)\n",
    "\n",
    "    # Add the final accuracy to the training statistics\n",
    "    train_stats['final_accuracy'] = final_acc\n",
    "    \n",
    "    return network, train_stats, list_train_stats\n",
    "    \n",
    "    "
   ],
   "id": "9df790ceaa19d5cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training Parameters\n",
    "TrainParams['strategy'] = 'un-freeze'\n",
    "\n",
    "# Train the network with gradually unfreezing layers\n",
    "TrainYN = False\n",
    "if TrainYN:\n",
    "    trained_net_S2, train_stats_S2, list_train_stats = TrainResNet18_S2(data_root, 'category', transform, TrainParams)\n"
   ],
   "id": "3cb040db77075577"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-Tuning with imbalanced classes\n",
    "- Check the Training behavior with class-imbalance\n",
    "- Try a strategy (e.g. weighted cross-entropy and/or over-sampling of minority classes) to compensate the imbalanced training set"
   ],
   "id": "91db389c5507fdcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "id": "80bb6d5c6e2b0f98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extend basic project --> B/A\n",
    "- FixMatch\n",
    "- https://github.com/google-research/fixmatch/blob/master/pseudo_label.py\n"
   ],
   "id": "3118596d49f9270c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation\n",
    "- weak augmentation\n",
    "- strong augmentation"
   ],
   "id": "d866cfbadae014c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_strong_augmentation(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0))], p=0.5),\n",
    "        transforms.RandomApply([transforms.RandomRotation(degrees=15)], p=0.5),\n",
    "        RandAugment(num_ops=2, magnitude=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_weak_augmentation(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Imagenet normalization\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "class TransformFixMatch:\n",
    "    \"\"\"Applies weak and strong augmentation to the same input image.\"\"\"\n",
    "    def __init__(self, image_size=224):\n",
    "        self.weak = get_weak_augmentation(image_size)\n",
    "        self.strong = get_strong_augmentation(image_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        weak_aug = self.weak(x)\n",
    "        strong_aug = self.strong(x)\n",
    "        return weak_aug, strong_aug\n",
    "\n",
    "\n",
    "# Example usage\n",
    "transform = TransformFixMatch(image_size=224)\n",
    "image = PIL.Image.open('datasets/oxford-iiit-pet/images/Abyssinian_1.jpg')\n",
    "weak_aug, strong_aug = transform(image)\n",
    "# Visualize the augmentations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(np.asarray(image))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[1].imshow(weak_aug.permute(1, 2, 0).numpy())\n",
    "axes[1].set_title('Weak Augmentation')\n",
    "axes[2].imshow(strong_aug.permute(1, 2, 0).numpy())\n",
    "axes[2].set_title('Strong Augmentation')\n",
    "plt.show()\n"
   ],
   "id": "ffb8c95bbdf58c5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the TransformFixMatch class (Visualize the Augmentation)",
   "id": "b8895453006551f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image_size = 224\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "# Define transforms\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "strong_transform = TransformFixMatch(image_size)  # RandAugment included\n",
    "\n",
    "# Print the transformations\n",
    "#print(\"Weak Transformations:\", weak_transform)\n",
    "#print(\"Strong Transformations:\", strong_transform.strong)\n",
    "\n",
    "# Extract # random pictures from the dataset\n",
    "no_pictures = 10\n",
    "ran_indices = random.sample(range(len(images2vis)), no_pictures)\n",
    "\n",
    "# Extract the images and labels\n",
    "ran_images = [de_normalize(images2vis[i][0]) for i in ran_indices]\n",
    "labels = [images2vis.dataset.classes[images2vis[i][1]] for i in ran_indices]\n",
    "images_raw = [F.to_pil_image(img) for img in ran_images]\n",
    "\n",
    "\n",
    "# Apply the transformations to the dataset\n",
    "images_weakaug = [de_normalize(weak_transform(img)) for img in images_raw]\n",
    "images_weakaug = [F.to_pil_image(img) for img in images_weakaug]\n",
    "images_strongaug = [de_normalize(strong_transform(img)[1]) if isinstance(strong_transform(img), tuple) \\\n",
    "    else strong_transform(img) for img in images_raw]\n",
    "images_strongaug = [F.to_pil_image(img) for img in images_strongaug]\n",
    "\n",
    "# Visualize the weak and strong augmentations\n",
    "fig, axes = plt.subplots(3, no_pictures+1, figsize=(20, 5))  # 3 rows, 6 columns (1 for labels, 5 for images)\n",
    "\n",
    "# Add row labels\n",
    "row_labels = [\"Raw Image:\", \"Weak Aug:\", \"Strong Aug:\"]\n",
    "for row, label in enumerate(row_labels):\n",
    "    axes[row, 0].text(0.5, 0.5, label, fontsize=20, ha='center', va='center')\n",
    "    axes[row, 0].axis('off')\n",
    "\n",
    "# Add images and headers\n",
    "for col, (raw, weak, strong, label) in enumerate(zip(images_raw, images_weakaug, images_strongaug, labels), start=1):\n",
    "    # First row: raw images with headers\n",
    "    axes[0, col].imshow(raw)\n",
    "    axes[0, col].set_title(label, fontsize=12)\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    # Second row: weak augmentations\n",
    "    axes[1, col].imshow(weak)\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "    # Third row: strong augmentations\n",
    "    axes[2, col].imshow(strong)\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "3c19b6db90acb09e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Perpare the Dataset\n",
    "- Split labelled / unlabelled dataset"
   ],
   "id": "c97f1c2a475577c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_labeled_unlabeled(dataset, num_labels_per_class, num_classes, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set random seed for reproducibility\n",
    "    \n",
    "    # Access the labels\n",
    "    try:\n",
    "        targets = np.array(dataset.targets)  # Use targets attribute if available\n",
    "    except AttributeError:\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])  # Fallback to accessing each sample\n",
    "    \n",
    "    labeled_indices = []\n",
    "    unlabeled_indices = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_indices = np.where(targets == class_idx)[0]\n",
    "        if len(class_indices) < num_labels_per_class:\n",
    "            raise ValueError(f\"Class {class_idx} has fewer samples than num_labels_per_class.\")\n",
    "        \n",
    "        np.random.shuffle(class_indices)\n",
    "        labeled_indices.extend(class_indices[:num_labels_per_class])\n",
    "        unlabeled_indices.extend(class_indices[num_labels_per_class:])\n",
    "    \n",
    "    return np.array(labeled_indices), np.array(unlabeled_indices)\n",
    "\n",
    "\n",
    "def load_train_val_test_data(data_root, target_types, transform, num_labels_per_class, num_classes, batch_size_labeld=32, batch_size_unlabeld=64, image_size=224):\n",
    "    # Common normalization values for ImageNet (can fine-tune later)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    # Define transforms\n",
    "    transform_FixMatch = TransformFixMatch(image_size)\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    # Load the full dataset\n",
    "    full_dataset = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='trainval',\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.OxfordIIITPet(\n",
    "        root=data_root,\n",
    "        split='test',\n",
    "        target_types=target_types,\n",
    "        transform=val_test_transform,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    # Split into labeled and unlabeled data\n",
    "    labeled_indices, unlabeled_indices = split_labeled_unlabeled(full_dataset, num_labels_per_class, num_classes)\n",
    "    \n",
    "    # Create labeled and unlabeled datasets\n",
    "    labeled_dataset = Subset(full_dataset, labeled_indices)\n",
    "    unlabeled_dataset = Subset(full_dataset, unlabeled_indices)\n",
    "    \n",
    "    # Apply transforms\n",
    "    labeled_dataset.dataset.transform = transform_FixMatch.weak # Use only weak augmentation for labeled data\n",
    "    unlabeled_dataset.dataset.transform = transform_FixMatch    # Use both weak and strong augmentations for unlabeled data\n",
    "    \n",
    "    # Optional: make a validation split out of labeled set\n",
    "    val_ratio = 0.2\n",
    "    val_size = int(val_ratio * len(labeled_dataset))\n",
    "    train_size = len(labeled_dataset) - val_size\n",
    "    labeled_dataset, val_dataset = random_split(labeled_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Dictionary to hold DataLoaders\n",
    "    dataloaders = {\n",
    "        'labeled': DataLoader(labeled_dataset, batch_size=batch_size_labeld, shuffle=True),\n",
    "        'unlabeled': DataLoader(unlabeled_dataset, batch_size=batch_size_unlabeld, shuffle=True),\n",
    "        'val': DataLoader(val_dataset, batch_size=batch_size_labeld, shuffle=False),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size_labeld, shuffle=False)\n",
    "    }\n",
    "    \n",
    "    return dataloaders\n"
   ],
   "id": "f11e937722969ddc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "58730aecb002b606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_fixmatch_loss(logits_x, targets_x, logits_u_w, logits_u_s, threshold=0.95, lambda_u=1.0):\n",
    "\n",
    "    \n",
    "    # Supervised loss\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss_x = criterion(logits_x, targets_x)\n",
    "    \n",
    "    # Generate pseudo labels from weakly augmented unlabeled data\n",
    "    pseudo_labels = torch.softmax(logits_u_w, dim=-1)\n",
    "    max_probs, targets_u = torch.max(pseudo_labels, dim=-1)\n",
    "    \n",
    "    # Mask to select high-confidence pseudo-labels\n",
    "    mask = max_probs.ge(threshold).float()\n",
    "    \n",
    "    # Unsupervised loss (only for confident predictions)\n",
    "    unsupervised_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    loss_u = (unsupervised_criterion(logits_u_s, targets_u) * mask).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = loss_x + lambda_u * loss_u\n",
    "    \n",
    "    return total_loss, loss_x, loss_u\n"
   ],
   "id": "8d2edc982032e657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop",
   "id": "249d211d4bdc5354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, device, epoch, lambda_u=1.0, threshold=0.95):\n",
    "    model.train()\n",
    "\n",
    "    # Track losses\n",
    "    running_loss = 0.0\n",
    "    running_supervised_loss = 0.0\n",
    "    running_unsupervised_loss = 0.0\n",
    "\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    for batch_idx, (inputs_x_ws, targets_x) in enumerate(labeled_loader):\n",
    "        try:\n",
    "            # Correctly unpack the unlabeled data\n",
    "            unlabeled_batch = next(unlabeled_iter)\n",
    "            inputs_u, _ = unlabeled_batch  # Unpack images and ignore labels\n",
    "            inputs_u_w, inputs_u_s = inputs_u  # Unpack weak and strong augmentations\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            unlabeled_batch = next(unlabeled_iter)\n",
    "            inputs_u, _ = unlabeled_batch\n",
    "            inputs_u_w, inputs_u_s = inputs_u\n",
    "\n",
    "        # Move data to device (GPU or CPU)\n",
    "        inputs_x, _ = inputs_x_ws # Unpack images (weak and strong augmentations) - Size: [batch_size, 3, 224, 224]\n",
    "        inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "        inputs_u_w, inputs_u_s = inputs_u_w.to(device), inputs_u_s.to(device)\n",
    "\n",
    "        # Get batch size\n",
    "        batch_size = inputs_x.size(0)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_x = model(inputs_x)\n",
    "        logits_u_w = model(inputs_u_w)\n",
    "        logits_u_s = model(inputs_u_s)\n",
    "\n",
    "        # Compute FixMatch loss\n",
    "        total_loss, loss_x, loss_u = compute_fixmatch_loss(\n",
    "            logits_x, targets_x, logits_u_w, logits_u_s,\n",
    "            threshold=threshold, lambda_u=lambda_u\n",
    "        )\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += total_loss.item()\n",
    "        running_supervised_loss += loss_x.item()\n",
    "        running_unsupervised_loss += loss_u.item()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch [{epoch}] Batch [{batch_idx}] \"\n",
    "                  f\"Loss: {total_loss.item():.4f} \"\n",
    "                  f\"Supervised Loss: {loss_x.item():.4f} \"\n",
    "                  f\"Unsupervised Loss: {loss_u.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(labeled_loader)\n",
    "    epoch_supervised_loss = running_supervised_loss / len(labeled_loader)\n",
    "    epoch_unsupervised_loss = running_unsupervised_loss / len(labeled_loader)\n",
    "\n",
    "    return epoch_loss, epoch_supervised_loss, epoch_unsupervised_loss\n"
   ],
   "id": "47fb82aca3513902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "df7b00111ae6f098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def accuracy(output, target, topk=(1,5)):\n",
    "    \"\"\"Computes the top-k accuracy\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)  # Top-k predictions\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            if isinstance(inputs, list):\n",
    "                inputs = inputs[0]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute top-k accuracies\n",
    "            top1, top5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            \n",
    "            top1_correct += top1.item() * inputs.size(0) / 100.0\n",
    "            top5_correct += top5.item() * inputs.size(0) / 100.0\n",
    "            total += inputs.size(0)\n",
    "    \n",
    "    top1_acc = 100.0 * top1_correct / total\n",
    "    top5_acc = 100.0 * top5_correct / total\n",
    "\n",
    "    print(f\"Validation Top-1 Accuracy: {top1_acc:.2f}% | Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    \n",
    "    return top1_acc, top5_acc\n"
   ],
   "id": "905c3fccb4a4f471"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to Visualize images from Dataloaders (for debugging)",
   "id": "438325df8e91ca07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize a batch of images\n",
    "def visualize_dataloader(dataloader, num_images=8):\n",
    "    # Get a batch of data\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    # Extract images and labels\n",
    "    print(f\"Type of images: {type(images)}\")\n",
    "    print(f\"Type of labels: {type(labels)}\")\n",
    "    if isinstance(images, list):\n",
    "        images = images[0]  # Use only the weakly augmented images\n",
    "    \n",
    "    # Create a grid of images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        img = de_normalize(images[i])\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    plt.show()\n"
   ],
   "id": "8e87b51ef2e57be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Full Training Script\n",
    "- Possibility to \"speed-up\": If more training is necessary:\n",
    "    - Since the best model is saved at every epoch (if it is better then the previous of course)\n",
    "    - then it can be loaded again (instead of initialization)\n",
    "    - and training can be proceeded"
   ],
   "id": "bd0757b96286c37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "data_root = \"datasets\"\n",
    "target_types = 'category'\n",
    "num_classes = 37  # Pet dataset (all breeds)\n",
    "num_labels_per_class = 40  # Number of labeled samples per class\n",
    "image_size = 224\n",
    "batch_size_labeled = 32\n",
    "batch_size_unlabeled = batch_size_labeled*5 # 5x more than labeled, typical for FixMatch\n",
    "num_epochs = 30\n",
    "threshold = 0.95\n",
    "lambda_u = 1.0  # weight for unlabeled loss\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Model\n",
    "model = Initialize_ResNet18(num_classes)  # 37 classes for pet breeds\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Data Loaders\n",
    "dataloaders = load_train_val_test_data(data_root, target_types, transform, num_labels_per_class, num_classes, batch_size_labeled, batch_size_unlabeled, image_size)\n",
    "labeled_loader = dataloaders['labeled']\n",
    "unlabeled_loader = dataloaders['unlabeled']\n",
    "validation_loader = dataloaders['val']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "# Visualize images from the Dataloaders\n",
    "debugYN = False\n",
    "if debugYN:\n",
    "    print(\"\\n--- Labeled Data ---\")\n",
    "    visualize_dataloader(dataloaders['labeled'], 10)\n",
    "    print(\"\\n--- Unlabeled Data ---\")\n",
    "    visualize_dataloader(dataloaders['unlabeled'], 10)\n",
    "    print(\"\\n--- Validation Data ---\")\n",
    "    visualize_dataloader(dataloaders['val'], 10)\n",
    "    print(\"\\n--- Test Data ---\")\n",
    "    visualize_dataloader(dataloaders['test'], 10)\n",
    "\n",
    "top1_list = []\n",
    "top5_list = []\n",
    "loss_list = []\n",
    "supervised_loss_list = []\n",
    "unsupervised_loss_list = []\n",
    "\n",
    "# Training Loop\n",
    "best_top1 = 0.0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{num_epochs} ---\")\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_supervised_loss, train_unsupervised_loss = train_fixmatch(\n",
    "        model, labeled_loader, unlabeled_loader, optimizer, device, epoch,\n",
    "        lambda_u=lambda_u, threshold=threshold\n",
    "    )\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Supervised: {train_supervised_loss:.4f} | Unsupervised: {train_unsupervised_loss:.4f}\")\n",
    "    loss_list.append(train_loss)\n",
    "    supervised_loss_list.append(train_supervised_loss)\n",
    "    unsupervised_loss_list.append(train_unsupervised_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    val_top1, val_top5 = evaluate(model, validation_loader, device)\n",
    "    top1_list.append(val_top1)\n",
    "    top5_list.append(val_top5)\n",
    "\n",
    "    # Save best model\n",
    "    if val_top1 > best_top1:\n",
    "        best_top1 = val_top1\n",
    "        torch.save(model.state_dict(), \"best_fixmatch_model.pth\")\n",
    "        print(\"Saved new best model!\")\n",
    "    else:\n",
    "        print(\"No improvement, not saving model.\")        \n",
    "        \n",
    "    "
   ],
   "id": "6d3d15aeacf71f51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Saved from Learning Network for 10 epochs (180 min)\n",
    "\n",
    "--- Epoch 1/10 ---\n",
    "Epoch [1] Batch [0] Loss: 3.9471 Supervised Loss: 3.9471 Unsupervised Loss: 0.0000\n",
    "Epoch [1] Batch [20] Loss: 2.1357 Supervised Loss: 1.9048 Unsupervised Loss: 0.2310\n",
    "Train Loss: 2.4848 | Supervised: 2.3746 | Unsupervised: 0.1102\n",
    "Validation Top-1 Accuracy: 14.86% | Top-5 Accuracy: 46.28%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 2/10 ---\n",
    "Epoch [2] Batch [0] Loss: 1.9900 Supervised Loss: 1.8160 Unsupervised Loss: 0.1740\n",
    "Epoch [2] Batch [20] Loss: 3.2791 Supervised Loss: 3.2272 Unsupervised Loss: 0.0519\n",
    "Train Loss: 2.9669 | Supervised: 2.8571 | Unsupervised: 0.1098\n",
    "Validation Top-1 Accuracy: 15.20% | Top-5 Accuracy: 47.64%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 3/10 ---\n",
    "Epoch [3] Batch [0] Loss: 2.3836 Supervised Loss: 2.3728 Unsupervised Loss: 0.0108\n",
    "Epoch [3] Batch [20] Loss: 2.5582 Supervised Loss: 2.3905 Unsupervised Loss: 0.1677\n",
    "Train Loss: 2.4857 | Supervised: 2.3802 | Unsupervised: 0.1055\n",
    "Validation Top-1 Accuracy: 26.69% | Top-5 Accuracy: 69.26%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 4/10 ---\n",
    "Epoch [4] Batch [0] Loss: 3.1559 Supervised Loss: 3.0505 Unsupervised Loss: 0.1054\n",
    "Epoch [4] Batch [20] Loss: 1.5976 Supervised Loss: 1.3893 Unsupervised Loss: 0.2083\n",
    "Train Loss: 1.9308 | Supervised: 1.7513 | Unsupervised: 0.1795\n",
    "Validation Top-1 Accuracy: 31.42% | Top-5 Accuracy: 72.64%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 5/10 ---\n",
    "Epoch [5] Batch [0] Loss: 2.4420 Supervised Loss: 2.1580 Unsupervised Loss: 0.2839\n",
    "Epoch [5] Batch [20] Loss: 1.0752 Supervised Loss: 0.9148 Unsupervised Loss: 0.1604\n",
    "Train Loss: 1.3770 | Supervised: 1.1924 | Unsupervised: 0.1846\n",
    "Validation Top-1 Accuracy: 43.24% | Top-5 Accuracy: 78.38%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 6/10 ---\n",
    "Epoch [6] Batch [0] Loss: 1.5669 Supervised Loss: 1.2905 Unsupervised Loss: 0.2764\n",
    "Epoch [6] Batch [20] Loss: 1.1614 Supervised Loss: 0.9400 Unsupervised Loss: 0.2214\n",
    "Train Loss: 1.3390 | Supervised: 1.1382 | Unsupervised: 0.2008\n",
    "Validation Top-1 Accuracy: 47.64% | Top-5 Accuracy: 79.39%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 7/10 ---\n",
    "Epoch [7] Batch [0] Loss: 0.9391 Supervised Loss: 0.7451 Unsupervised Loss: 0.1940\n",
    "Epoch [7] Batch [20] Loss: 1.1811 Supervised Loss: 0.9402 Unsupervised Loss: 0.2409\n",
    "Train Loss: 1.1847 | Supervised: 0.9285 | Unsupervised: 0.2562\n",
    "Validation Top-1 Accuracy: 46.62% | Top-5 Accuracy: 81.76%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 8/10 ---\n",
    "Epoch [8] Batch [0] Loss: 0.9729 Supervised Loss: 0.8089 Unsupervised Loss: 0.1640\n",
    "Epoch [8] Batch [20] Loss: 0.5816 Supervised Loss: 0.3195 Unsupervised Loss: 0.2621\n",
    "Train Loss: 0.8694 | Supervised: 0.6335 | Unsupervised: 0.2359\n",
    "Validation Top-1 Accuracy: 53.04% | Top-5 Accuracy: 86.82%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 9/10 ---\n",
    "Epoch [9] Batch [0] Loss: 0.6071 Supervised Loss: 0.4218 Unsupervised Loss: 0.1853\n",
    "Epoch [9] Batch [20] Loss: 1.2063 Supervised Loss: 0.7612 Unsupervised Loss: 0.4451\n",
    "Train Loss: 0.7340 | Supervised: 0.4695 | Unsupervised: 0.2645\n",
    "Validation Top-1 Accuracy: 52.03% | Top-5 Accuracy: 86.49%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 10/10 ---\n",
    "Epoch [10] Batch [0] Loss: 0.5246 Supervised Loss: 0.3115 Unsupervised Loss: 0.2130\n",
    "Epoch [10] Batch [20] Loss: 0.5426 Supervised Loss: 0.2863 Unsupervised Loss: 0.2563\n",
    "Train Loss: 0.6224 | Supervised: 0.3653 | Unsupervised: 0.2571\n",
    "Validation Top-1 Accuracy: 59.46% | Top-5 Accuracy: 89.19%\n",
    "Saved new best model!\n",
    "\n",
    "\n",
    "\n",
    "### Saved from Learning Network for 30 epochs (1120 min)\n",
    "--- Epoch 1/30 ---\n",
    "Epoch [1] Batch [0] Loss: 3.8736 Supervised Loss: 3.8736 Unsupervised Loss: 0.0000\n",
    "Epoch [1] Batch [20] Loss: 1.7233 Supervised Loss: 1.6408 Unsupervised Loss: 0.0825\n",
    "Train Loss: 2.4875 | Supervised: 2.3792 | Unsupervised: 0.1084\n",
    "Validation Top-1 Accuracy: 4.73% | Top-5 Accuracy: 29.73%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 2/30 ---\n",
    "Epoch [2] Batch [0] Loss: 3.6865 Supervised Loss: 3.5388 Unsupervised Loss: 0.1477\n",
    "Epoch [2] Batch [20] Loss: 2.6268 Supervised Loss: 2.6151 Unsupervised Loss: 0.0117\n",
    "Train Loss: 3.1425 | Supervised: 3.0523 | Unsupervised: 0.0902\n",
    "Validation Top-1 Accuracy: 21.96% | Top-5 Accuracy: 54.05%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 3/30 ---\n",
    "Epoch [3] Batch [0] Loss: 2.5924 Supervised Loss: 2.4876 Unsupervised Loss: 0.1048\n",
    "Epoch [3] Batch [20] Loss: 2.0461 Supervised Loss: 1.9342 Unsupervised Loss: 0.1119\n",
    "Train Loss: 2.6453 | Supervised: 2.5342 | Unsupervised: 0.1110\n",
    "Validation Top-1 Accuracy: 25.00% | Top-5 Accuracy: 65.54%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 4/30 ---\n",
    "Epoch [4] Batch [0] Loss: 2.4073 Supervised Loss: 2.2402 Unsupervised Loss: 0.1672\n",
    "Epoch [4] Batch [20] Loss: 2.4360 Supervised Loss: 2.2563 Unsupervised Loss: 0.1798\n",
    "Train Loss: 2.0720 | Supervised: 1.9001 | Unsupervised: 0.1718\n",
    "Validation Top-1 Accuracy: 20.61% | Top-5 Accuracy: 64.19%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 5/30 ---\n",
    "Epoch [5] Batch [0] Loss: 1.6327 Supervised Loss: 1.4678 Unsupervised Loss: 0.1649\n",
    "Epoch [5] Batch [20] Loss: 1.2737 Supervised Loss: 1.0817 Unsupervised Loss: 0.1919\n",
    "Train Loss: 1.7122 | Supervised: 1.5182 | Unsupervised: 0.1940\n",
    "Validation Top-1 Accuracy: 37.16% | Top-5 Accuracy: 77.03%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 6/30 ---\n",
    "Epoch [6] Batch [0] Loss: 1.1685 Supervised Loss: 0.9762 Unsupervised Loss: 0.1923\n",
    "Epoch [6] Batch [20] Loss: 1.2245 Supervised Loss: 0.9365 Unsupervised Loss: 0.2880\n",
    "Train Loss: 1.2306 | Supervised: 0.9989 | Unsupervised: 0.2318\n",
    "Validation Top-1 Accuracy: 47.97% | Top-5 Accuracy: 84.80%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 7/30 ---\n",
    "Epoch [7] Batch [0] Loss: 0.9433 Supervised Loss: 0.8093 Unsupervised Loss: 0.1340\n",
    "Epoch [7] Batch [20] Loss: 1.0662 Supervised Loss: 0.7543 Unsupervised Loss: 0.3119\n",
    "Train Loss: 0.9773 | Supervised: 0.7708 | Unsupervised: 0.2065\n",
    "Validation Top-1 Accuracy: 44.59% | Top-5 Accuracy: 77.03%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 8/30 ---\n",
    "Epoch [8] Batch [0] Loss: 0.7726 Supervised Loss: 0.5455 Unsupervised Loss: 0.2272\n",
    "Epoch [8] Batch [20] Loss: 0.8409 Supervised Loss: 0.4525 Unsupervised Loss: 0.3883\n",
    "Train Loss: 0.8694 | Supervised: 0.6291 | Unsupervised: 0.2403\n",
    "Validation Top-1 Accuracy: 50.00% | Top-5 Accuracy: 86.49%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 9/30 ---\n",
    "Epoch [9] Batch [0] Loss: 0.4127 Supervised Loss: 0.3259 Unsupervised Loss: 0.0868\n",
    "Epoch [9] Batch [20] Loss: 0.4005 Supervised Loss: 0.1971 Unsupervised Loss: 0.2033\n",
    "Train Loss: 0.6480 | Supervised: 0.4106 | Unsupervised: 0.2374\n",
    "Validation Top-1 Accuracy: 56.08% | Top-5 Accuracy: 91.55%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 10/30 ---\n",
    "Epoch [10] Batch [0] Loss: 0.6422 Supervised Loss: 0.3292 Unsupervised Loss: 0.3130\n",
    "Epoch [10] Batch [20] Loss: 0.3551 Supervised Loss: 0.1424 Unsupervised Loss: 0.2127\n",
    "Train Loss: 0.5664 | Supervised: 0.3130 | Unsupervised: 0.2533\n",
    "Validation Top-1 Accuracy: 52.36% | Top-5 Accuracy: 87.50%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 11/30 ---\n",
    "Epoch [11] Batch [0] Loss: 0.5699 Supervised Loss: 0.3514 Unsupervised Loss: 0.2185\n",
    "Epoch [11] Batch [20] Loss: 0.4310 Supervised Loss: 0.1126 Unsupervised Loss: 0.3184\n",
    "Train Loss: 0.5214 | Supervised: 0.2715 | Unsupervised: 0.2498\n",
    "Validation Top-1 Accuracy: 56.08% | Top-5 Accuracy: 92.23%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 12/30 ---\n",
    "Epoch [12] Batch [0] Loss: 0.6357 Supervised Loss: 0.3383 Unsupervised Loss: 0.2974\n",
    "Epoch [12] Batch [20] Loss: 0.5724 Supervised Loss: 0.3671 Unsupervised Loss: 0.2053\n",
    "Train Loss: 0.5270 | Supervised: 0.2754 | Unsupervised: 0.2516\n",
    "Validation Top-1 Accuracy: 56.76% | Top-5 Accuracy: 90.54%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 13/30 ---\n",
    "Epoch [13] Batch [0] Loss: 0.2837 Supervised Loss: 0.0658 Unsupervised Loss: 0.2179\n",
    "Epoch [13] Batch [20] Loss: 0.4661 Supervised Loss: 0.2340 Unsupervised Loss: 0.2322\n",
    "Train Loss: 0.4237 | Supervised: 0.1831 | Unsupervised: 0.2406\n",
    "Validation Top-1 Accuracy: 61.49% | Top-5 Accuracy: 90.88%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 14/30 ---\n",
    "Epoch [14] Batch [0] Loss: 0.2799 Supervised Loss: 0.0393 Unsupervised Loss: 0.2406\n",
    "Epoch [14] Batch [20] Loss: 0.3266 Supervised Loss: 0.1697 Unsupervised Loss: 0.1569\n",
    "Train Loss: 0.3888 | Supervised: 0.1614 | Unsupervised: 0.2274\n",
    "Validation Top-1 Accuracy: 63.85% | Top-5 Accuracy: 91.22%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 15/30 ---\n",
    "Epoch [15] Batch [0] Loss: 0.1836 Supervised Loss: 0.0632 Unsupervised Loss: 0.1203\n",
    "Epoch [15] Batch [20] Loss: 0.4839 Supervised Loss: 0.2540 Unsupervised Loss: 0.2299\n",
    "Train Loss: 0.3377 | Supervised: 0.1370 | Unsupervised: 0.2006\n",
    "Validation Top-1 Accuracy: 64.86% | Top-5 Accuracy: 89.86%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 16/30 ---\n",
    "Epoch [16] Batch [0] Loss: 0.3505 Supervised Loss: 0.0802 Unsupervised Loss: 0.2703\n",
    "Epoch [16] Batch [20] Loss: 0.2344 Supervised Loss: 0.0694 Unsupervised Loss: 0.1650\n",
    "Train Loss: 0.3171 | Supervised: 0.1065 | Unsupervised: 0.2107\n",
    "Validation Top-1 Accuracy: 61.15% | Top-5 Accuracy: 90.20%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 17/30 ---\n",
    "Epoch [17] Batch [0] Loss: 0.0950 Supervised Loss: 0.0136 Unsupervised Loss: 0.0814\n",
    "Epoch [17] Batch [20] Loss: 0.2724 Supervised Loss: 0.0316 Unsupervised Loss: 0.2408\n",
    "Train Loss: 0.2763 | Supervised: 0.0874 | Unsupervised: 0.1888\n",
    "Validation Top-1 Accuracy: 65.20% | Top-5 Accuracy: 89.86%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 18/30 ---\n",
    "Epoch [18] Batch [0] Loss: 0.3213 Supervised Loss: 0.0584 Unsupervised Loss: 0.2629\n",
    "Epoch [18] Batch [20] Loss: 0.2418 Supervised Loss: 0.1062 Unsupervised Loss: 0.1356\n",
    "Train Loss: 0.2293 | Supervised: 0.0609 | Unsupervised: 0.1684\n",
    "Validation Top-1 Accuracy: 68.24% | Top-5 Accuracy: 93.58%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 19/30 ---\n",
    "Epoch [19] Batch [0] Loss: 0.1781 Supervised Loss: 0.0238 Unsupervised Loss: 0.1543\n",
    "Epoch [19] Batch [20] Loss: 0.3267 Supervised Loss: 0.0818 Unsupervised Loss: 0.2449\n",
    "Train Loss: 0.2546 | Supervised: 0.0718 | Unsupervised: 0.1828\n",
    "Validation Top-1 Accuracy: 69.59% | Top-5 Accuracy: 93.92%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 20/30 ---\n",
    "Epoch [20] Batch [0] Loss: 0.1900 Supervised Loss: 0.0152 Unsupervised Loss: 0.1748\n",
    "Epoch [20] Batch [20] Loss: 0.1728 Supervised Loss: 0.0202 Unsupervised Loss: 0.1527\n",
    "Train Loss: 0.2210 | Supervised: 0.0654 | Unsupervised: 0.1556\n",
    "Validation Top-1 Accuracy: 70.61% | Top-5 Accuracy: 93.24%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 21/30 ---\n",
    "Epoch [21] Batch [0] Loss: 0.2909 Supervised Loss: 0.0242 Unsupervised Loss: 0.2667\n",
    "Epoch [21] Batch [20] Loss: 0.1598 Supervised Loss: 0.0323 Unsupervised Loss: 0.1275\n",
    "Train Loss: 0.2825 | Supervised: 0.0892 | Unsupervised: 0.1933\n",
    "Validation Top-1 Accuracy: 67.57% | Top-5 Accuracy: 93.58%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 22/30 ---\n",
    "Epoch [22] Batch [0] Loss: 0.2988 Supervised Loss: 0.0776 Unsupervised Loss: 0.2212\n",
    "Epoch [22] Batch [20] Loss: 0.4599 Supervised Loss: 0.1522 Unsupervised Loss: 0.3077\n",
    "Train Loss: 0.2169 | Supervised: 0.0459 | Unsupervised: 0.1710\n",
    "Validation Top-1 Accuracy: 69.59% | Top-5 Accuracy: 93.58%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 23/30 ---\n",
    "Epoch [23] Batch [0] Loss: 0.1339 Supervised Loss: 0.0312 Unsupervised Loss: 0.1028\n",
    "Epoch [23] Batch [20] Loss: 0.4162 Supervised Loss: 0.0232 Unsupervised Loss: 0.3930\n",
    "Train Loss: 0.2279 | Supervised: 0.0652 | Unsupervised: 0.1626\n",
    "Validation Top-1 Accuracy: 66.89% | Top-5 Accuracy: 92.57%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 24/30 ---\n",
    "Epoch [24] Batch [0] Loss: 0.2604 Supervised Loss: 0.1861 Unsupervised Loss: 0.0743\n",
    "Epoch [24] Batch [20] Loss: 0.3112 Supervised Loss: 0.2053 Unsupervised Loss: 0.1059\n",
    "Train Loss: 0.2168 | Supervised: 0.0616 | Unsupervised: 0.1552\n",
    "Validation Top-1 Accuracy: 68.92% | Top-5 Accuracy: 93.92%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 25/30 ---\n",
    "Epoch [25] Batch [0] Loss: 0.2240 Supervised Loss: 0.0446 Unsupervised Loss: 0.1795\n",
    "Epoch [25] Batch [20] Loss: 0.1643 Supervised Loss: 0.0121 Unsupervised Loss: 0.1522\n",
    "Train Loss: 0.1627 | Supervised: 0.0206 | Unsupervised: 0.1421\n",
    "Validation Top-1 Accuracy: 69.26% | Top-5 Accuracy: 93.92%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 26/30 ---\n",
    "Epoch [26] Batch [0] Loss: 0.0986 Supervised Loss: 0.0068 Unsupervised Loss: 0.0918\n",
    "Epoch [26] Batch [20] Loss: 0.1254 Supervised Loss: 0.0145 Unsupervised Loss: 0.1108\n",
    "Train Loss: 0.1403 | Supervised: 0.0268 | Unsupervised: 0.1136\n",
    "Validation Top-1 Accuracy: 71.96% | Top-5 Accuracy: 94.59%\n",
    "Saved new best model!\n",
    "\n",
    "--- Epoch 27/30 ---\n",
    "Epoch [27] Batch [0] Loss: 0.1497 Supervised Loss: 0.0433 Unsupervised Loss: 0.1064\n",
    "Epoch [27] Batch [20] Loss: 0.1140 Supervised Loss: 0.0050 Unsupervised Loss: 0.1090\n",
    "Train Loss: 0.1546 | Supervised: 0.0198 | Unsupervised: 0.1349\n",
    "Validation Top-1 Accuracy: 68.92% | Top-5 Accuracy: 93.92%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 28/30 ---\n",
    "Epoch [28] Batch [0] Loss: 0.1507 Supervised Loss: 0.0043 Unsupervised Loss: 0.1464\n",
    "Epoch [28] Batch [20] Loss: 0.2406 Supervised Loss: 0.0158 Unsupervised Loss: 0.2248\n",
    "Train Loss: 0.1820 | Supervised: 0.0405 | Unsupervised: 0.1415\n",
    "Validation Top-1 Accuracy: 69.59% | Top-5 Accuracy: 93.92%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 29/30 ---\n",
    "Epoch [29] Batch [0] Loss: 0.1270 Supervised Loss: 0.0020 Unsupervised Loss: 0.1249\n",
    "Epoch [29] Batch [20] Loss: 0.1054 Supervised Loss: 0.0136 Unsupervised Loss: 0.0919\n",
    "Train Loss: 0.1586 | Supervised: 0.0284 | Unsupervised: 0.1302\n",
    "Validation Top-1 Accuracy: 69.93% | Top-5 Accuracy: 93.58%\n",
    "No improvement, not saving model.\n",
    "\n",
    "--- Epoch 30/30 ---\n",
    "Epoch [30] Batch [0] Loss: 0.1412 Supervised Loss: 0.0456 Unsupervised Loss: 0.0956\n",
    "Epoch [30] Batch [20] Loss: 0.1549 Supervised Loss: 0.0298 Unsupervised Loss: 0.1251\n",
    "Train Loss: 0.1522 | Supervised: 0.0271 | Unsupervised: 0.1251\n",
    "Validation Top-1 Accuracy: 72.97% | Top-5 Accuracy: 95.27%\n",
    "Saved new best model!\n",
    "\n"
   ],
   "id": "cea3cdcc183efb91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualizing the Top 1, Top 5 and Loss Trend",
   "id": "9d1ef6e05797163e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize Trend of Top-1 and Top-5 Accuracy\n",
    "def visualize_accuracy_trend(top1_list, top5_list):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(top1_list, label='Top-1 Accuracy', marker='o')\n",
    "    plt.plot(top5_list, label='Top-5 Accuracy', marker='o')\n",
    "    plt.title('Top-1 and Top-5 Accuracy Trend')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# Visualize Loss Trend\n",
    "def visualize_loss_trend(loss_list, supervised_loss_list, unsupervised_loss_list):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(loss_list, label='Total Loss', marker='o')\n",
    "    plt.plot(supervised_loss_list, label='Supervised Loss', marker='o')\n",
    "    plt.plot(unsupervised_loss_list, label='Unsupervised Loss', marker='o')\n",
    "    plt.title('Loss Trend')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Visualize Accuracy and Loss Trends\n",
    "visualize_accuracy_trend(top1_list, top5_list)\n",
    "visualize_loss_trend(loss_list, supervised_loss_list, unsupervised_loss_list)\n",
    "\n",
    "\n",
    "# Save the loss, top-1 and top-5 list to csv file\n",
    "filename = 'fixmatch_training_stats.csv'\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Top-1 Accuracy': top1_list,\n",
    "    'Top-5 Accuracy': top5_list,\n",
    "    'Total Loss': loss_list,\n",
    "    'Supervised Loss': supervised_loss_list,\n",
    "    'Unsupervised Loss': unsupervised_loss_list\n",
    "})\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(filename):\n",
    "    # Append to the existing file\n",
    "    df.to_csv(filename, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Create a new file\n",
    "    df.to_csv(filename, index=False)\n",
    "    "
   ],
   "id": "57030f4b5e60d739"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
